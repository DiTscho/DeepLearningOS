{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_02 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= Path('../PCImages')\n",
    "path2fm= '../FeatureMatsMerged/TheGreatCollection.txt'\n",
    "path2colnames= 'FeatureMatIndex.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56657, 105)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm= fm_from_txt(path2fm, path2colnames)\n",
    "fm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "      <th>label3</th>\n",
       "      <th>ind</th>\n",
       "      <th>wallcrash</th>\n",
       "      <th>temp</th>\n",
       "      <th>LP</th>\n",
       "      <th>...</th>\n",
       "      <th>err_peKV_ElRel_2s</th>\n",
       "      <th>E_aKV_ElRel_2s</th>\n",
       "      <th>eta_aKV_ElRel_2s</th>\n",
       "      <th>Act_aKV_ElRel_2s</th>\n",
       "      <th>err_aKV_ElRel_2s</th>\n",
       "      <th>E_aeKV_ElRel_2s</th>\n",
       "      <th>E2_aeKV_ElRel_2s</th>\n",
       "      <th>eta_aeKV_ElRel_2s</th>\n",
       "      <th>Act_aeKV_ElRel_2s</th>\n",
       "      <th>err_aeKV_ElRel_2s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>180322140137</td>\n",
       "      <td>180322.0</td>\n",
       "      <td>140137.0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.00</td>\n",
       "      <td>875.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000840</td>\n",
       "      <td>34.801664</td>\n",
       "      <td>33.466240</td>\n",
       "      <td>3.135231e-05</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>3.518287e+01</td>\n",
       "      <td>1091.754900</td>\n",
       "      <td>36.509799</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.000819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>180322140233</td>\n",
       "      <td>180322.0</td>\n",
       "      <td>140233.0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.01</td>\n",
       "      <td>875.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>29.482185</td>\n",
       "      <td>20.546622</td>\n",
       "      <td>2.180137e-07</td>\n",
       "      <td>0.002530</td>\n",
       "      <td>1.576053e-07</td>\n",
       "      <td>64.210986</td>\n",
       "      <td>111.861860</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>180322140601</td>\n",
       "      <td>180322.0</td>\n",
       "      <td>140601.0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>38.956936</td>\n",
       "      <td>17.98</td>\n",
       "      <td>875.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028127</td>\n",
       "      <td>17.579015</td>\n",
       "      <td>3.970008</td>\n",
       "      <td>1.079131e+00</td>\n",
       "      <td>0.012464</td>\n",
       "      <td>1.876191e+01</td>\n",
       "      <td>210.278410</td>\n",
       "      <td>4.268777</td>\n",
       "      <td>1.056558</td>\n",
       "      <td>0.012754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>180322140713</td>\n",
       "      <td>180322.0</td>\n",
       "      <td>140713.0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.99</td>\n",
       "      <td>875.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>62.245363</td>\n",
       "      <td>39.791365</td>\n",
       "      <td>2.516519e-01</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>5.171595e+01</td>\n",
       "      <td>993.694250</td>\n",
       "      <td>50.203118</td>\n",
       "      <td>0.320308</td>\n",
       "      <td>0.000632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>180322140852</td>\n",
       "      <td>180322.0</td>\n",
       "      <td>140852.0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.99</td>\n",
       "      <td>875.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002178</td>\n",
       "      <td>28.424611</td>\n",
       "      <td>9.381201</td>\n",
       "      <td>2.280459e-08</td>\n",
       "      <td>0.001762</td>\n",
       "      <td>2.870110e+01</td>\n",
       "      <td>1567.120300</td>\n",
       "      <td>9.746181</td>\n",
       "      <td>0.004185</td>\n",
       "      <td>0.001797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id      date      time  label1  label2  label3  ind  wallcrash  \\\n",
       "0  180322140137  180322.0  140137.0       0   100.0     0.0  1.0   0.000000   \n",
       "1  180322140233  180322.0  140233.0       0   100.0     0.0  2.0   0.000000   \n",
       "2  180322140601  180322.0  140601.0       0   100.0     0.0  3.0  38.956936   \n",
       "3  180322140713  180322.0  140713.0       0   100.0     0.0  4.0   0.000000   \n",
       "4  180322140852  180322.0  140852.0       0   100.0     0.0  5.0   0.000000   \n",
       "\n",
       "    temp     LP  ...  err_peKV_ElRel_2s  E_aKV_ElRel_2s  eta_aKV_ElRel_2s  \\\n",
       "0  18.00  875.0  ...           0.000840       34.801664         33.466240   \n",
       "1  18.01  875.0  ...           0.000397       29.482185         20.546622   \n",
       "2  17.98  875.0  ...           0.028127       17.579015          3.970008   \n",
       "3  17.99  875.0  ...           0.000750       62.245363         39.791365   \n",
       "4  17.99  875.0  ...           0.002178       28.424611          9.381201   \n",
       "\n",
       "   Act_aKV_ElRel_2s  err_aKV_ElRel_2s  E_aeKV_ElRel_2s  E2_aeKV_ElRel_2s  \\\n",
       "0      3.135231e-05          0.000837     3.518287e+01       1091.754900   \n",
       "1      2.180137e-07          0.002530     1.576053e-07         64.210986   \n",
       "2      1.079131e+00          0.012464     1.876191e+01        210.278410   \n",
       "3      2.516519e-01          0.000616     5.171595e+01        993.694250   \n",
       "4      2.280459e-08          0.001762     2.870110e+01       1567.120300   \n",
       "\n",
       "   eta_aeKV_ElRel_2s  Act_aeKV_ElRel_2s  err_aeKV_ElRel_2s  \n",
       "0          36.509799           0.000338           0.000819  \n",
       "1         111.861860           0.000031           0.000343  \n",
       "2           4.268777           1.056558           0.012754  \n",
       "3          50.203118           0.320308           0.000632  \n",
       "4           9.746181           0.004185           0.001797  \n",
       "\n",
       "[5 rows x 105 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols= ['id',\n",
    " 'date',\n",
    " 'time',\n",
    " 'label1',\n",
    " 'label2',\n",
    " 'label3',\n",
    " 'ind',\n",
    " 'wallcrash',\n",
    " 'temp',\n",
    " 'LP',\n",
    " 'HL',\n",
    " 't_stretch',\n",
    " 't_relax',\n",
    " 'framecut',\n",
    " 'fps',\n",
    " 'medium',\n",
    " 'passage',\n",
    " 'ms_ch1',\n",
    " 'mf_ch1',\n",
    " 'mf_real_ch1',\n",
    " 'cv_nn_ch1',\n",
    " 'cvn_ch1',\n",
    " 'ms_ch2',\n",
    " 'mf_ch2',\n",
    " 'mf_real_ch2',\n",
    " 'cv_nn_ch2',\n",
    " 'cvn_ch2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_row(fm, img_id, cols2drop):\n",
    "    return torch.cuda.FloatTensor(np.array(fm.loc[fm['id'].isin([img_id])].drop(columns=cols2drop)).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellDataSet(Dataset):\n",
    "    def __init__(self, path, fm, cols2drop= cols, label = 'label1', cell_phase = None, trfms=None, \n",
    "                 random_sample=None):\n",
    "        self.fm = fm\n",
    "        self.cols2drop= cols2drop\n",
    "        self.y = self.fm[label]\n",
    "        self.id2label= id2label(self.fm.id, self.y)\n",
    "        if cell_phase is None:\n",
    "            self.data_files = get_filtered_files(path, img_ids=self.fm.id, labels=self.y) \n",
    "        else: \n",
    "            self.data_files= [\n",
    "                f for f in get_filtered_files(path, img_ids=self.fm.id, labels=self.y) if cell_phase in str(f)\n",
    "            ]\n",
    "        if random_sample is not None: self.data_files= random.sample(self.data_files, random_sample)\n",
    "        self.trfms = get_trfms(trfms)\n",
    "        \n",
    "    def __getindex__(self, idx):\n",
    "        return load_file(self.data_files[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        img_name  = self.data_files[idx]\n",
    "        img_id, phase = split_fn(img_name)\n",
    "        label=          self.id2label[img_id]\n",
    "        image =         PIL.Image.open(img_name)\n",
    "        fm_row=         get_row(self.fm, img_id, self.cols2drop)\n",
    "\n",
    "        if self.trfms:\n",
    "            image = self.trfms(image)\n",
    "\n",
    "        return image, fm_row, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds= CellDataSet(path, fm, label = 'label1', cell_phase = None, \n",
    "               trfms= [CenterCrop(size=350), \n",
    "                       transforms.Grayscale(num_output_channels=1), \n",
    "                       ToFloatTensor()], \n",
    "               random_sample=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([78])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc0= Resize(350).cuda()\n",
    "        self.cnn = models.resnet34(pretrained=True).cuda()\n",
    "        self.cnn.conv1= nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False).cuda()\n",
    "        self.cnn.fc= nn.Linear(in_features=512, out_features=50).cuda()\n",
    "        \n",
    "        self.fc1 = nn.Linear(78+50, 30).cuda()\n",
    "        self.fc2 = nn.Linear(30, 7).cuda()\n",
    "        \n",
    "    def forward(self, image, data):\n",
    "        x1 = self.cnn(self.fc0(image)) #(1,20)\n",
    "        x2 = data                      #(1,78)\n",
    "        x = torch.cat((x1, x2), dim=1) #(1,98)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, dataset, model):\n",
    "        self.ds= dataset \n",
    "        self.img_size, self.c= self.ds[0][0].shape[0], len(np.unique(self.ds.y))\n",
    "        self.model= model #get_model(model.cuda(), image_size=self.img_size, c=self.c)\n",
    "        self.loss= nn.CrossEntropyLoss()\n",
    "        \n",
    "    def fit(self, epochs=1, bs=32, lr = 1e-5):\n",
    "        opt= torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        train_dl, valid_dl= ds2dls(self.ds, bs=bs)\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            for img_xb, data_xb, yb in train_dl:\n",
    "                img_xb=  img_xb.to(device)\n",
    "                data_xb= data_xb.to(device)\n",
    "                yb= yb.to(device)\n",
    "                loss = self.loss(self.model(img_xb, data_xb), yb)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                opt.zero_grad()\n",
    "\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                tot_loss,tot_acc = 0.,0.\n",
    "                for img_xb, data_xb, yb in valid_dl:\n",
    "                    img_xb=  img_xb.to(device)\n",
    "                    data_xb= data_xb.to(device)\n",
    "                    yb= yb.to(device)\n",
    "                    pred = self.model(img_xb, data_xb)\n",
    "                    pred= pred.to(device)\n",
    "                    tot_loss += self.loss(pred, yb)\n",
    "                    tot_acc  += accuracy(pred,yb)\n",
    "            nv = len(valid_dl)\n",
    "            print(epoch, tot_loss/nv, tot_acc/nv)\n",
    "        #return tot_loss/nv, tot_acc/nv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(533.1910, device='cuda:0') tensor(0.1100, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "ds= CellDataSet(path, fm, label = 'label1', cell_phase = None, \n",
    "               trfms= [CenterCrop(size=350), \n",
    "                       transforms.Grayscale(num_output_channels=1), \n",
    "                       ToFloatTensor()], \n",
    "               random_sample=1000)\n",
    "learn= Learner(ds, MyModel())\n",
    "learn.fit(1, bs=8, lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model for tabular data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularModel(nn.Module):\n",
    "    \"Basic model for tabular data.\"\n",
    "    def __init__(self, emb_szs, n_cont:int, out_sz:int, layers, ps=None,\n",
    "                 emb_drop:float=0., y_range=None, use_bn:bool=True, bn_final:bool=False):\n",
    "        super().__init__()\n",
    "        ps = ifnone(ps, [0]*len(layers))\n",
    "        ps = listify(ps, layers)\n",
    "        self.embeds = nn.ModuleList([nn.Embedding(ni, nf) for ni,nf in emb_szs]) #type: torch.nn.modules.container.ModuleList\n",
    "        self.emb_drop = nn.Dropout(emb_drop) #type: torch.nn.modules.dropout.Dropout\n",
    "        self.bn_cont = nn.BatchNorm1d(n_cont) #type torch.nn.modules.batchnorm.BatchNorm1d\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeds) # n_emb = 17 , type: int\n",
    "        self.n_emb,self.n_cont,self.y_range = n_emb,n_cont,y_range\n",
    "        sizes = [n_emb + n_cont] + layers + [out_sz] #typeL list, len: 4\n",
    "        actns = [nn.ReLU(inplace=True) for _ in range(len(sizes)-2)] + [None] #type: list, len: 3.  the last in None because we finish with linear\n",
    "        layers = []\n",
    "        for i,(n_in,n_out,dp,act) in enumerate(zip(sizes[:-1],sizes[1:],[0.]+ps,actns)):\n",
    "            layers += bn_drop_lin(n_in, n_out, bn=use_bn and i!=0, p=dp, actn=act)\n",
    "        if bn_final: layers.append(nn.BatchNorm1d(sizes[-1]))\n",
    "        self.layers = nn.Sequential(*layers) #type: torch.nn.modules.container.Sequential\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        if self.n_emb != 0:\n",
    "            x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)] #take the embedding list and grab an embedding and pass in our single row of data.        \n",
    "            x = torch.cat(x, 1) # concatenate it on dim 1 ## remeber that the len is the batch size\n",
    "            x = self.emb_drop(x) # pass it through a dropout layer\n",
    "        if self.n_cont != 0:\n",
    "            x_cont = self.bn_cont(x_cont) # batchnorm1d\n",
    "            x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont # combine the categircal and continous variables on dim 1\n",
    "        x = self.layers(x)\n",
    "        if self.y_range is not None:\n",
    "            x = (self.y_range[1]-self.y_range[0]) * torch.sigmoid(x) + self.y_range[0] # deal with y_range\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
