{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_04 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= Path('../PCImages')\n",
    "path2fm= '../FeatureMatsMerged/TheGreatCollection.txt'\n",
    "path2colnames= 'FeatureMatIndex.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56657, 105)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm= fm_from_txt(path2fm, path2colnames)\n",
    "fm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_filtered=get_filtered_cols(fm)\n",
    "len(cols_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def normalize(df): return (df-df.mean())/df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pkbar\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make it faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import fnmatch\n",
    "Path.ls = lambda x: list(x.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CellMixUpDataSet(Dataset):\n",
    "    def __init__(self, path, fm, cols2keep, label = 'label1', mixup_cs=[.4, .3, .3], trfms=None, random_sample=None):\n",
    "        self.fm= fm\n",
    "        if random_sample is not None: self.fm= self.fm.sample(n=random_sample, random_state=0)\n",
    "        self.cols2keep= cols2keep\n",
    "        self.fm[self.cols2keep]= normalize(self.fm[self.cols2keep])\n",
    "        self.ids, self.ys= zip(*id2label(self.fm.id, self.fm[label]).items())\n",
    "        self.data_files= path.ls()\n",
    "        self.trfms = get_trfms(trfms)\n",
    "        self.cs= mixup_cs\n",
    "        self.l= len(self.ids[0])+1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    def get_row(self, img_id):\n",
    "        x_np= np.array(self.fm.loc[self.fm['id'].isin([img_id])][self.cols2keep]).flatten() \n",
    "        return torch.cuda.FloatTensor(x_np)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        try:\n",
    "            img_id=    self.ids[i]\n",
    "            label=     self.ys[i]\n",
    "            img_names= [f for f in self.data_files if (img_id.split('_')[0] in f.name) \n",
    "                        and (img_id.split('_')[1] in f.name)]\n",
    "            image =    sum([c*io.imread(name) for c,name in zip(self.cs, img_names)])\n",
    "            fm_row=    self.get_row(img_id)\n",
    "            image=     Image.fromarray(np.uint8(image))\n",
    "            if self.trfms:\n",
    "                image = self.trfms(image)\n",
    "        except IndexError:\n",
    "            return None\n",
    "        return image, fm_row, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def collate_fn(batch):\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    return torch.utils.data.dataloader.default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ds2dls(ds, bs, val_split = 0.2, shuffle_ds = True, random_seed = 0):\n",
    "\n",
    "    ds_size = len(ds)\n",
    "    inds = list(range(ds_size))\n",
    "    split = int(np.floor(val_split * ds_size))\n",
    "    if shuffle_ds:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(inds)\n",
    "    train_inds, val_inds = inds[split:], inds[:split]\n",
    "\n",
    "    ts = SubsetRandomSampler(train_inds)\n",
    "    vs = SubsetRandomSampler(val_inds)\n",
    "\n",
    "    train_dl = DataLoader(ds, batch_size=bs, collate_fn=collate_fn, sampler=ts)\n",
    "    valid_dl = DataLoader(ds, batch_size=bs, collate_fn=collate_fn, sampler=vs)\n",
    "\n",
    "    return train_dl, valid_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Learner():\n",
    "    def __init__(self, dataset, model, bs=8):\n",
    "        self.bs=bs\n",
    "        self.ds= dataset \n",
    "        self.img_size, self.c= self.ds[0][0].shape[0], len(np.unique(self.ds.ys))\n",
    "        self.model= model #get_model(model.cuda(), image_size=self.img_size, c=self.c)\n",
    "        self.loss= nn.CrossEntropyLoss()\n",
    "        self.train_dl, self.valid_dl= ds2dls(self.ds, bs=self.bs)\n",
    "        \n",
    "    def fit(self, epochs=1, lr = 1e-5):\n",
    "        opt= torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        for epoch in range(epochs):\n",
    "            kbar = pkbar.Kbar(target=len(self.ds)//self.bs, epoch=epoch, \n",
    "                              num_epochs=epochs, width=1, always_stateful=False)\n",
    "            self.model.train()\n",
    "            for i, [img_xb, data_xb, yb] in enumerate(self.train_dl):\n",
    "                img_xb=  img_xb.to(device)\n",
    "                data_xb= data_xb.to(device)\n",
    "                yb= yb.to(device)\n",
    "                pred = self.model(img_xb, data_xb)\n",
    "                loss = self.loss(pred, yb)\n",
    "                acc  = accuracy(pred,yb)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                opt.zero_grad()\n",
    "                \n",
    "                kbar.update(i, values=[(\"train loss\", loss), (\"train acc\", acc)])\n",
    "\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                tot_loss,tot_acc = 0.,0.\n",
    "                for i, [img_xb, data_xb, yb] in enumerate(self.valid_dl):\n",
    "                    img_xb=  img_xb.to(device)\n",
    "                    data_xb= data_xb.to(device)\n",
    "                    yb= yb.to(device)\n",
    "                    pred = self.model(img_xb, data_xb)\n",
    "                    pred= pred.to(device)\n",
    "                    loss      = self.loss(pred, yb)\n",
    "                    tot_loss += loss\n",
    "                    acc       = accuracy(pred,yb)\n",
    "                    tot_acc  += acc\n",
    "                    \n",
    "                    kbar.update(i, values=[(\"valid loss\", loss), (\"valid acc\", acc)])\n",
    "            nv = len(self.valid_dl)\n",
    "            print('\\n Total vali loss and accuracy: ', tot_loss.data.cpu()/nv, tot_acc.data.cpu()/nv)\n",
    "        #return tot_loss/nv, tot_acc/nv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds= CellMixUpDataSet(path, fm, cols_filtered, label = 'label1', \n",
    "                mixup_cs=[.5, .25, .25],\n",
    "                trfms= [CenterCrop(size=350), \n",
    "                       transforms.Grayscale(num_output_channels=1), \n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize(mean=[0.18], std=[0.12])\n",
    "                       ], \n",
    "                random_sample=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3\n",
      "0/5 [.] - ETA: 0s - train loss: 1.9603 - train acc: 0.0000e+00 - valid loss: -6.0909 - valid acc: 0.0000e+00\n",
      " Total vali loss and accuracy:  tensor(2.0303) tensor(0.)\n",
      "Epoch: 2/3\n",
      "0/5 [.] - ETA: 0s - train loss: 1.9931 - train acc: 0.0000e+00 - valid loss: -6.3963 - valid acc: 0.0000e+00\n",
      " Total vali loss and accuracy:  tensor(2.1321) tensor(0.)\n",
      "Epoch: 3/3\n",
      "0/5 [.] - ETA: 0s - train loss: 1.9541 - train acc: 0.0000e+00 - valid loss: -6.6809 - valid acc: 0.0000e+00\n",
      " Total vali loss and accuracy:  tensor(2.2270) tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "learn= Learner(ds, CombinedModel(50,10, p=0.5), bs=2)\n",
    "learn.fit(3, lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(learn.model, 'models/')\n",
    "torch.save(learn.train_dl, 'models/train_dl.pth')\n",
    "torch.save(learn.valid_dl, 'models/valid_dl.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
